{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SD_corr_sim4_ResNet152.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNglvUIwzv9f9l+jutuILZu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuillermoPuebla/Cogsci2019Piecemeal/blob/master/SD_corr_sim4_ResNet152.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zWofOm4CTbl"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Input\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticates the Colab machine and also the TPU using your\n",
        "# credentials so that they can access your private GCS buckets.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ZZV1JsnSOcO0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TPU initialization\n",
        "# Note that the tpu argument to tf.distribute.cluster_resolver.TPUClusterResolver \n",
        "# is a special address just for Colab. If you are running your code on Google \n",
        "# Compute Engine (GCE), you should instead pass in the name of your Cloud TPU.\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "metadata": {
        "id": "4H5uDL_3Oaqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define TPU strategy\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "OUIZkXdZOas0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "RhY_ByDmOavL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "def make_ResNet50_sd_classifier(\n",
        "    base_weights='imagenet',\n",
        "    base_trainable=False,\n",
        "    gap=True,\n",
        "    secondary_outputs=True\n",
        "    ):\n",
        "    # Define input tensor\n",
        "    image = Input((128, 128, 3))\n",
        "    # Base pre-trained model\n",
        "    if base_weights == 'imagenet':\n",
        "        base_model = ResNet152(weights='imagenet', include_top=False, input_tensor=image)\n",
        "    elif base_weights is None:\n",
        "        base_model = ResNet152(weights=None, include_top=False, input_tensor=image)\n",
        "    elif base_weights == 'TU-Berlin':\n",
        "        # Get weights from sketch classifier base.\n",
        "        sketch_b, sketch_m = make_ResNet50_sketch_classifier(weights=None, trainable=True)\n",
        "        sketch_m.load_weights('cogsci_sims/sketches/ResNet50_classifier_sketches_weights.10-2.92.hdf5')\n",
        "        base_weights = []\n",
        "        for layer in sketch_b.layers:\n",
        "            base_weights.append(layer.get_weights())\n",
        "        # Set weights of base model.\n",
        "        base_model = ResNet152(weights=None, include_top=False, input_tensor=image)\n",
        "        i = 0\n",
        "        for layer in base_model.layers:\n",
        "            layer.set_weights(base_weights[i])\n",
        "            i += 1\n",
        "    # Freeze the if necesarry base_model\n",
        "    base_model.trainable = base_trainable\n",
        "    # Add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    if gap:\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "    else:\n",
        "        x = Flatten()(x)\n",
        "    # Add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    # Add logistic layer\n",
        "    predictions = Dense(1, activation='sigmoid', name='sd')(x)\n",
        "    if secondary_outputs:\n",
        "        rel_pos = Dense(1, activation='sigmoid', name='rel_pos')(x)\n",
        "        # This is the model we will train\n",
        "        model = Model(inputs=base_model.input, outputs=[predictions, rel_pos])\n",
        "    else:\n",
        "        # This is the model we will train\n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return base_model, model\n"
      ],
      "metadata": {
        "id": "4KEFk_IqOaxi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "def get_dataset(\n",
        "    batch_size,\n",
        "    tfrecord_dir,\n",
        "    autotune_settings,\n",
        "    is_training=True,\n",
        "    process_img=True,\n",
        "    sd_sample_weight=True,\n",
        "    relnet=False\n",
        "    ):\n",
        "    # Load dataset\n",
        "    raw_image_dataset = tf.data.TFRecordDataset(\n",
        "        tfrecord_dir,\n",
        "        num_parallel_reads=autotune_settings\n",
        "        )\n",
        "    # Define example reading function\n",
        "    def read_tfrecord(serialized_example):\n",
        "        # Create a dictionary describing the features\n",
        "        feature_description = {\n",
        "            'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "            'coordinates': tf.io.FixedLenFeature([], tf.string),\n",
        "            'relative_position': tf.io.FixedLenFeature([], tf.int64)}\n",
        "        # Parse example\n",
        "        example = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "        # Cast label to int64\n",
        "        label = example['label']\n",
        "        label = tf.cast(label, tf.int64)\n",
        "        # Get image\n",
        "        image = tf.image.decode_png(example['image_raw'])\n",
        "        # Ensure shape dimensions are constant\n",
        "        image = tf.reshape(image, [128, 128, 3])\n",
        "        # Process image\n",
        "        if process_img:\n",
        "            image = tf.cast(image, tf.float64)\n",
        "            image /= 255.0\n",
        "            # Sample-wise center image\n",
        "            mean = tf.reduce_mean(image)\n",
        "            image -= mean\n",
        "            # Sample-wise std normalization\n",
        "            std = tf.math.reduce_std(image)\n",
        "            image /= std\n",
        "        # Cast relative position to int64\n",
        "        rel_pos = example['relative_position']\n",
        "        rel_pos = tf.cast(rel_pos, tf.int64)\n",
        "        # Sample weights\n",
        "        sd_w = tf.constant(1, dtype=tf.int64) if sd_sample_weight else tf.constant(0, dtype=tf.int64)\n",
        "        rp_w = tf.constant(1, dtype=tf.int64)\n",
        "        if relnet:\n",
        "            question = tf.constant([1, 0], dtype=tf.int64) if sd_sample_weight else tf.constant([0, 1], dtype=tf.int64)\n",
        "            rp_w = tf.constant(0, dtype=tf.int64) if sd_sample_weight else tf.constant(1, dtype=tf.int64)\n",
        "            return (image, question), (label, rel_pos), (sd_w, rp_w)\n",
        "        else:\n",
        "            return image, (label, rel_pos), (sd_w, rp_w)\n",
        "    # Parse dataset\n",
        "    dataset = raw_image_dataset.map(\n",
        "        read_tfrecord, \n",
        "        num_parallel_calls=autotune_settings\n",
        "        )\n",
        "    # Always shuffle for simplicity\n",
        "    dataset = dataset.shuffle(7000, reshuffle_each_iteration=True)\n",
        "    # Infinite dataset to avoid the potential last partial batch in each epoch\n",
        "    if is_training:\n",
        "        dataset = dataset.repeat()\n",
        "    if batch_size is not None:\n",
        "        dataset = dataset.batch(batch_size).prefetch(autotune_settings)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def get_master_dataset(\n",
        "    autotune_settings, \n",
        "    batch_size,\n",
        "    ds_dir,\n",
        "    dont_include,\n",
        "    process_img=True\n",
        "    ):\n",
        "    \"\"\"Builds dataset that samples each batch from one of the training datasets\n",
        "    assiging a same-different sample weight of 1 for all but the dont_include\n",
        "    dataset and a relative-position sample weight 1 for all other datasets.\"\"\"\n",
        "    \n",
        "    # Make datasets and append if it is not the dont_include dataset.\n",
        "    datasets = []\n",
        "    \n",
        "    # Original\n",
        "    original_sw_w = False if dont_include == 'Original' else True\n",
        "    ds_original = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/original_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=original_sw_w)\n",
        "    datasets.append(ds_original)\n",
        "\n",
        "    # Regular\n",
        "    regular_sw_w = False if dont_include == 'Regular' else True\n",
        "    ds_regular = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/regular_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=regular_sw_w\n",
        "        )\n",
        "    datasets.append(ds_regular)\n",
        "\n",
        "    # Irregular\n",
        "    irregular_sw_w = False if dont_include == 'Irregular' else True\n",
        "    ds_irregular = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/irregular_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=irregular_sw_w\n",
        "        )\n",
        "    datasets.append(ds_irregular)\n",
        "\n",
        "    # Open\n",
        "    open_sw_w = False if dont_include == 'Open' else True\n",
        "    ds_open = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/open_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=open_sw_w\n",
        "        )\n",
        "    datasets.append(ds_open)\n",
        "\n",
        "    # Wider line\n",
        "    wider_sw_w = False if dont_include == 'Wider line' else True\n",
        "    ds_wider = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/wider_line_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=wider_sw_w\n",
        "        )\n",
        "    datasets.append(ds_wider)\n",
        "\n",
        "    # Scrambled\n",
        "    scrambled_sw_w = False if dont_include == 'Scrambled' else True\n",
        "    ds_scrambled = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/scrambled_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=scrambled_sw_w\n",
        "        )\n",
        "    datasets.append(ds_scrambled)\n",
        "\n",
        "    # Random color\n",
        "    random_sw_w = False if dont_include == 'Random color' else True\n",
        "    ds_random = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/random_color_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=random_sw_w\n",
        "        )\n",
        "    datasets.append(ds_random)\n",
        "\n",
        "    # Filled\n",
        "    filled_sw_w = False if dont_include == 'Filled' else True\n",
        "    ds_filled = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/filled_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=filled_sw_w\n",
        "        )\n",
        "    datasets.append(ds_filled)\n",
        "\n",
        "    # Lines\n",
        "    lines_sw_w = False if dont_include == 'Lines' else True\n",
        "    ds_lines = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/lines_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=lines_sw_w\n",
        "        )\n",
        "    datasets.append(ds_lines)\n",
        "\n",
        "    # Arrows\n",
        "    arrows_sw_w = False if dont_include == 'Arrows' else True\n",
        "    ds_arrows = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/arrows_train.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=True,\n",
        "        process_img=process_img,\n",
        "        sd_sample_weight=arrows_sw_w\n",
        "        )\n",
        "    datasets.append(ds_arrows)\n",
        "    \n",
        "    # Note. No need to oversample the original dataset in simulation 4 as\n",
        "    # the model is trained on the same/different task in all datasets except\n",
        "    # on the one that is going to be tested.\n",
        "    choice_dataset = tf.data.Dataset.range(len(datasets)).repeat()\n",
        "    \n",
        "    return tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\n"
      ],
      "metadata": {
        "id": "lj9byNToOaz3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training functions\n",
        "def fine_tune_model(\n",
        "    strategy,\n",
        "    train_ds,\n",
        "    val_ds,\n",
        "    save_name,\n",
        "    model_name,\n",
        "    epochs_top,\n",
        "    epochs,\n",
        "    steps_per_epoch,\n",
        "    validation_steps,\n",
        "    n=10,\n",
        "    lr=0.0001,\n",
        "    base_weights='imagenet',\n",
        "    gap=True\n",
        "    ):\n",
        "    with strategy.scope():\n",
        "        for i in range(n):\n",
        "            # Define model\n",
        "            base_model, model = make_ResNet50_sd_classifier(\n",
        "                base_weights=base_weights,\n",
        "                base_trainable=False,\n",
        "                gap=gap,\n",
        "                secondary_outputs=True\n",
        "                )\n",
        "            # Compile\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(0.0003),\n",
        "                loss={'sd': 'binary_crossentropy', 'rel_pos': 'binary_crossentropy'},\n",
        "                metrics={'sd': 'binary_accuracy', 'rel_pos': 'binary_accuracy'}\n",
        "                )\n",
        "            # Train\n",
        "            filename = save_name + model_name +'_run_' + str(i) + '_log.csv'\n",
        "            history_logger = tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "            model.fit(\n",
        "                train_ds,\n",
        "                epochs=epochs_top,\n",
        "                verbose=0,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                validation_data=val_ds,\n",
        "                validation_steps=validation_steps,\n",
        "                callbacks=[history_logger]\n",
        "                )\n",
        "            # Unfreeze Resnet50\n",
        "            base_model.trainable = True\n",
        "            # Re-compile\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                loss={'sd': 'binary_crossentropy', 'rel_pos': 'binary_crossentropy'},\n",
        "                metrics={'sd': 'binary_accuracy', 'rel_pos': 'binary_accuracy'})\n",
        "            # Train\n",
        "            model.fit(\n",
        "                train_ds,\n",
        "                epochs=epochs,\n",
        "                verbose=0,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                validation_data=val_ds,\n",
        "                validation_steps=validation_steps,\n",
        "                callbacks=[history_logger]\n",
        "                )\n",
        "            # Save weights\n",
        "            weights_name = save_name + model_name + '_instance_' + str(i) + '.hdf5'\n",
        "            model.save_weights(weights_name)\n",
        "    return\n",
        "\n",
        "def train_in_all_datasets(\n",
        "    strategy,\n",
        "    autotune_settings,\n",
        "    ds_dir,\n",
        "    sim_dir,\n",
        "    epochs_top,\n",
        "    epochs,\n",
        "    steps_per_epoch,\n",
        "    validation_steps,\n",
        "    n=10,\n",
        "    lr=0.0001,\n",
        "    batch_size=64):\n",
        "    # Define all master datasets\n",
        "    # no_irregular_ds = get_master_dataset(batch_size=batch_size, dont_include='Irregular', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    # no_regular_ds = get_master_dataset(batch_size=batch_size, dont_include='Regular', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    # no_open_ds = get_master_dataset(batch_size=batch_size, dont_include='Open', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    # no_wider_ds = get_master_dataset(batch_size=batch_size, dont_include='Wider line', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    no_scrambled_ds = get_master_dataset(batch_size=batch_size, dont_include='Scrambled', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    no_random_ds = get_master_dataset(batch_size=batch_size, dont_include='Random color', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    no_filled_ds = get_master_dataset(batch_size=batch_size, dont_include='Filled', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    no_lines_ds = get_master_dataset(batch_size=batch_size, dont_include='Lines', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    no_arrows_ds = get_master_dataset(batch_size=batch_size, dont_include='Arrows', process_img=True, autotune_settings=autotune_settings, ds_dir=ds_dir)\n",
        "    # Validation dataset\n",
        "    # val_ds = get_dataset(\n",
        "    #     batch_size=batch_size,\n",
        "    #     tfrecord_dir=ds_dir+'/original_val.tfrecords',\n",
        "    #     is_training=False,\n",
        "    #     process_img=True\n",
        "    #     )\n",
        "    # Train model in each dataset 10 times.\n",
        "    ds_and_names = [\n",
        "                    # (no_irregular_ds, sim_dir+'/ResNet152_no_irregular/'),\n",
        "                    # (no_regular_ds, sim_dir+'/ResNet152_no_regular/'),\n",
        "                    # (no_open_ds, sim_dir+'/ResNet152_no_open/'),\n",
        "                    # (no_wider_ds, sim_dir+'/ResNet152_no_wider/'),\n",
        "                    (no_scrambled_ds, sim_dir+'/ResNet152_no_scrambled/'),\n",
        "                    (no_random_ds, sim_dir+'/ResNet152_no_random/'),\n",
        "                    (no_filled_ds, sim_dir+'/ResNet152_no_filled/'),\n",
        "                    (no_lines_ds, sim_dir+'/ResNet152_no_lines/'),\n",
        "                    (no_arrows_ds, sim_dir+'/ResNet152_no_arrows/')\n",
        "                    ]\n",
        "    for ds, name in ds_and_names:\n",
        "        fine_tune_model(\n",
        "            strategy=strategy,\n",
        "            train_ds=ds,\n",
        "            val_ds=None,\n",
        "            save_name=name,\n",
        "            model_name='ResNet152',\n",
        "            epochs_top=epochs_top,\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_steps=None,\n",
        "            n=n,\n",
        "            lr=lr,\n",
        "            base_weights='imagenet',\n",
        "            gap=True\n",
        "            )\n",
        "    return\n"
      ],
      "metadata": {
        "id": "0v4lsFLFOa2P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test functions\n",
        "def test_model_auc(\n",
        "    ds, \n",
        "    weights_dir,\n",
        "    model,\n",
        "    model_name, \n",
        "    condition\n",
        "    ):\n",
        "    \"\"\"Tests 10 versions of a single model in a single condition using the area under the ROC curve. \n",
        "    Args:\n",
        "        ds: dataset with cases from the 'same' and 'different' classes.\n",
        "        weights_dir: directory of models weights to test.\n",
        "        model_name: model name. String.\n",
        "        condition: condition name. String.\n",
        "    Returns:\n",
        "        A list with test data: model_name, condition, area under the ROC curve.\n",
        "    \"\"\"\n",
        "    # Get list of weights from path\n",
        "    weights_list = [f for f in listdir(weights_dir) if f.endswith('.hdf5')]\n",
        "    weights_paths = [join(weights_dir, f) for f in weights_list if isfile(join(weights_dir, f))]\n",
        "    \n",
        "    # Test each model\n",
        "    models_data = []\n",
        "    for w_path in weights_paths:\n",
        "        model.load_weights(w_path)\n",
        "        metrics = model.evaluate(ds, verbose=2)\n",
        "        # model.metrics_names = ['loss', 'sd_loss', 'rel_pos_loss', 'sd_auc_1', 'rel_pos_auc_1']\n",
        "        models_data.append([model_name, condition, 'Same-Different', metrics[3]])\n",
        "        models_data.append([model_name, condition, 'Relative position', metrics[4]])\n",
        "    \n",
        "    return models_data\n",
        "\n",
        "def test_models_all_ds_auc(\n",
        "    strategy,\n",
        "    autotune_settings,\n",
        "    batch_size, \n",
        "    sim_dir, \n",
        "    ds_dir\n",
        "    ):\n",
        "    # Load same/different datasets.\n",
        "    datasets = []\n",
        "    \n",
        "    # Regular\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/regular_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Irregular\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/irregular_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Open\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/open_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "\n",
        "    # Wider line\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/wider_line_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Scrambled\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/scrambled_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Random color\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/random_color_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Filled\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/filled_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Lines\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/lines_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    # Arrows\n",
        "    ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=f'{ds_dir}/arrows_test.tfrecords',\n",
        "        autotune_settings=autotune_settings,\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        sd_sample_weight=True\n",
        "        )\n",
        "    datasets.append(ds)\n",
        "    \n",
        "    conditions = [\n",
        "                  'Regular', \n",
        "                  'Irregular', \n",
        "                  'Open', \n",
        "                  'Wider Line', \n",
        "                  'Scrambled', \n",
        "                  'Random Color', \n",
        "                  'Filled', \n",
        "                  'Lines', \n",
        "                  'Arrows'\n",
        "                  ]\n",
        "    weights_dirs = [\n",
        "                    sim_dir+'/ResNet152_no_regular', \n",
        "                    sim_dir+'/ResNet152_no_irregular', \n",
        "                    sim_dir+'/ResNet152_no_open',\n",
        "                    sim_dir+'/ResNet152_no_wider', \n",
        "                    sim_dir+'/ResNet152_no_scrambled', \n",
        "                    sim_dir+'/ResNet152_no_random',\n",
        "                    sim_dir+'/ResNet152_no_filled', \n",
        "                    sim_dir+'/ResNet152_no_lines', \n",
        "                    sim_dir+'/ResNet152_no_arrows'\n",
        "                    ]\n",
        "    with strategy.scope():\n",
        "        # Define model architecture\n",
        "        base_model, model = make_ResNet50_sd_classifier(base_weights=None, base_trainable=True, gap=True)\n",
        "        # Compile\n",
        "        model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(0.0003),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=[tf.keras.metrics.AUC()]\n",
        "                )\n",
        "        results = []\n",
        "        for ds, condition, w_dir in zip(datasets, conditions, weights_dirs):\n",
        "            # Fine-tune, imagenet, GAP.\n",
        "            ft_im_gap = test_model_auc(\n",
        "                ds=ds,\n",
        "                weights_dir=w_dir,\n",
        "                model=model,\n",
        "                model_name='ResNet152',\n",
        "                condition=condition\n",
        "                )\n",
        "            results.extend(ft_im_gap)\n",
        "    \n",
        "    return pd.DataFrame(results, columns=['Model', 'Condition', 'Task', 'AUC'])\n"
      ],
      "metadata": {
        "id": "UkVmMAfjOa4k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation functions\n",
        "def test_model_val_auc(ds_val, ds_name, weights_dir, model, model_name, condition):\n",
        "    \"\"\"\n",
        "    Tests 10 versions of a single model in a single condition using the area under the ROC curve. \n",
        "    Args:\n",
        "        ds_val: single dataset with cases from the 'same' and 'different' classes.\n",
        "        ds_name: name of the dataset being tested. String.\n",
        "        weights_dir: directory of models weights to test.\n",
        "        model_name: model name. String.\n",
        "        condition: name of the dataset in which the model was not trained on. String.\n",
        "    Returns:\n",
        "        A list with test data: model_name, condition, ds_name, area under the ROC curve.\n",
        "    \"\"\"\n",
        "    # Get list of weights from path\n",
        "    weights_list = [f for f in listdir(weights_dir) if f.endswith('.hdf5')]\n",
        "    weights_paths = [join(weights_dir, f) for f in weights_list if isfile(join(weights_dir, f))]\n",
        "\n",
        "    # Test each model\n",
        "    models_data = []\n",
        "    for w_path in weights_paths:\n",
        "        model.load_weights(w_path)\n",
        "        metrics_val = model.evaluate(ds_val, verbose=2)\n",
        "        # model.metrics_names = ['loss', 'sd_loss', 'rel_pos_loss', 'sd_auc_1', 'rel_pos_auc_1']\n",
        "        models_data.append([model_name, condition, ds_name, 'Same-Different', metrics_val[3]])\n",
        "        models_data.append([model_name, condition, ds_name, 'Relative position', metrics_val[4]])\n",
        "\n",
        "    return models_data\n",
        "\n",
        "def test_model_val_auc_on_datastets(\n",
        "    names_and_datasets,\n",
        "    weights_dir,\n",
        "    condition,\n",
        "    model,\n",
        "    model_name='ResNet152'\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Test 10 models in weights_dir on each dataset in names_and_datasets.\n",
        "    Args:\n",
        "        names_and_datasets: list of tuples (name, dataset).\n",
        "        weights_dir: directory with the weights of 10 runs of a model.\n",
        "        condition: name of the dataset the model was not trained on.\n",
        "        model_name: model name. String.\n",
        "    Returns:\n",
        "        A list with test data: model_name, condition, ds_name, area under the ROC curve.\n",
        "    \"\"\"\n",
        "    datasets_data = []\n",
        "    for ds_name, ds in names_and_datasets:\n",
        "        models_data = test_model_val_auc(\n",
        "            ds_val=ds, \n",
        "            ds_name=ds_name, \n",
        "            weights_dir=weights_dir, \n",
        "            model=model, \n",
        "            model_name='ResNet152', \n",
        "            condition=condition\n",
        "            )\n",
        "        datasets_data.extend(models_data)\n",
        "    return datasets_data \n",
        "\n",
        "def test_all_conditions_val_auc(\n",
        "    strategy,\n",
        "    autotune_settings,\n",
        "    batch_size, \n",
        "    ds_dir,\n",
        "    sim_dir,\n",
        "    relnet=False\n",
        "    ):\n",
        "    original_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/original_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    regular_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/regular_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    irregular_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/irregular_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    open_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/open_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    wider_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/wider_line_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    scrambled_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/scrambled_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    random_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/random_color_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    filled_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/filled_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    lines_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/lines_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    arrows_ds = get_dataset(\n",
        "        batch_size=batch_size,\n",
        "        tfrecord_dir=ds_dir+'/arrows_val.tfrecords',\n",
        "        is_training=False,\n",
        "        process_img=True,\n",
        "        relnet=relnet,\n",
        "        autotune_settings=autotune_settings\n",
        "        )\n",
        "    names_and_ds = [\n",
        "                    ('Original', original_ds),\n",
        "                    ('Regular', regular_ds),\n",
        "                    ('Irregular', irregular_ds),\n",
        "                    ('Open', open_ds),\n",
        "                    ('Wider Line', wider_ds),\n",
        "                    ('Scrambled', scrambled_ds),\n",
        "                    ('Random Color', random_ds),\n",
        "                    ('Filled', filled_ds),\n",
        "                    ('Lines', lines_ds),\n",
        "                    ('Arrows', arrows_ds)\n",
        "                    ]\n",
        "    weights_dirs = [\n",
        "                    sim_dir+'/ResNet152_no_regular', \n",
        "                    sim_dir+'/ResNet152_no_irregular', \n",
        "                    sim_dir+'/ResNet152_no_open',\n",
        "                    sim_dir+'/ResNet152_no_wider', \n",
        "                    sim_dir+'/ResNet152_no_scrambled',\n",
        "                    sim_dir+'/ResNet152_no_random',\n",
        "                    sim_dir+'/ResNet152_no_filled', \n",
        "                    sim_dir+'/ResNet152_no_lines', \n",
        "                    sim_dir+'/ResNet152_no_arrows'\n",
        "                    ]\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Define model\n",
        "        base_model, model = make_ResNet50_sd_classifier(base_weights=None, base_trainable=True, gap=True)\n",
        "        # Compile: set metric to auc (default is area under the ROC curve)\n",
        "        model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(0.0003),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=[tf.keras.metrics.AUC()]\n",
        "                )\n",
        "        results = []\n",
        "        for w_dir in weights_dirs:\n",
        "            # Identify ds to ignore for condition\n",
        "            if w_dir.endswith('no_regular'):\n",
        "                ds_to_ignore = 'Regular'\n",
        "            elif w_dir.endswith('no_irregular'):\n",
        "                ds_to_ignore = 'Irregular'\n",
        "            elif w_dir.endswith('no_open'):\n",
        "                ds_to_ignore = 'Open'\n",
        "            elif w_dir.endswith('no_wider'):\n",
        "                ds_to_ignore = 'Wider Line'\n",
        "            elif w_dir.endswith('no_scrambled'):\n",
        "                ds_to_ignore = 'Scrambled'\n",
        "            elif w_dir.endswith('no_random'):\n",
        "                ds_to_ignore = 'Random Color'\n",
        "            elif w_dir.endswith('no_filled'):\n",
        "                ds_to_ignore = 'Filled'\n",
        "            elif w_dir.endswith('no_lines'):\n",
        "                ds_to_ignore = 'Lines'\n",
        "            elif w_dir.endswith('no_arrows'):\n",
        "                ds_to_ignore = 'Arrows'\n",
        "            else:\n",
        "                raise ValueError('Unrecognised dataset name!')\n",
        "            print(ds_to_ignore)\n",
        "            condition_names_and_ds = [x for x in names_and_ds if x[0] != ds_to_ignore]\n",
        "            condition_data = test_model_val_auc_on_datastets(\n",
        "                names_and_datasets=condition_names_and_ds,\n",
        "                weights_dir=w_dir,\n",
        "                condition=ds_to_ignore,\n",
        "                model=model,\n",
        "                model_name='ResNet152'\n",
        "                )\n",
        "            results.extend(condition_data)\n",
        "    return pd.DataFrame(results, columns=['Model', 'Condition', 'Testing data', 'Task', 'AUC'])\n"
      ],
      "metadata": {
        "id": "WL_gpemiOa7K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS_TOP = 5\n",
        "EPOCHS = 15 #5+13\n",
        "BATCH_SIZE = 64\n",
        "STEPS_PER_EPOCH = (28000 // BATCH_SIZE)\n",
        "VALIDATION_STEPS = 2800 // BATCH_SIZE"
      ],
      "metadata": {
        "id": "QbH-XsliObAI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ResNet50 classifier\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "train_in_all_datasets(\n",
        "    strategy=strategy,\n",
        "    autotune_settings=AUTO,\n",
        "    ds_dir='gs://original-ds',\n",
        "    sim_dir='/content/gdrive/MyDrive/projects/same_diff_corrections/simulation_4',\n",
        "    epochs_top=EPOCHS_TOP,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_steps=None,\n",
        "    n=10,\n",
        "    lr=0.0001,\n",
        "    batch_size=BATCH_SIZE\n",
        "    )\n"
      ],
      "metadata": {
        "id": "20e2JAisObCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "df = test_models_all_ds_auc(\n",
        "    strategy=strategy,\n",
        "    autotune_settings=AUTO,\n",
        "    batch_size=560, \n",
        "    sim_dir='/content/gdrive/MyDrive/projects/same_diff_corrections/simulation_4', \n",
        "    ds_dir='gs://original-ds'\n",
        "    )\n",
        "df.to_csv('/content/gdrive/MyDrive/projects/same_diff_corrections/simulation_4/sim_4_ResNet152_relpos_test_auc.csv')"
      ],
      "metadata": {
        "id": "8mCo2aT-ObFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "df = test_all_conditions_val_auc(\n",
        "    strategy=strategy,\n",
        "    autotune_settings=AUTO,\n",
        "    batch_size=560, \n",
        "    ds_dir='gs://original-ds',\n",
        "    sim_dir='/content/gdrive/MyDrive/projects/same_diff_corrections/simulation_4',\n",
        "    relnet=False\n",
        "    )\n",
        "df.to_csv('/content/gdrive/MyDrive/projects/same_diff_corrections/simulation_4/sim_4_ResNet152_relpos_val_auc.csv')"
      ],
      "metadata": {
        "id": "0MtVlQtqObHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BtbWPxWuObJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}